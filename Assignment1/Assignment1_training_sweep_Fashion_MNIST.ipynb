{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment1_training_sweep_Fashion_MNIST.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOnuNAXI8RL324b9+6QQKt3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RahulSundar/100-Days-Of-ML-Code/blob/master/Assignment1/Assignment1_training_sweep_Fashion_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AABknAuK_8UD",
        "outputId": "8d4bb73e-6c05-409e-b741-e88547b92a05"
      },
      "source": [
        "!pip install wandb\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wandb\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/33/ae/79374d2b875e638090600eaa2a423479865b7590c53fb78e8ccf6a64acb1/wandb-0.10.22-py2.py3-none-any.whl (2.0MB)\n",
            "\u001b[K     |████████████████████████████████| 2.0MB 9.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.12.4)\n",
            "Collecting subprocess32>=3.5.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz (97kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 8.1MB/s \n",
            "\u001b[?25hCollecting shortuuid>=0.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/25/a6/2ecc1daa6a304e7f1b216f0896b26156b78e7c38e1211e9b798b4716c53d/shortuuid-1.0.1-py3-none-any.whl\n",
            "Collecting sentry-sdk>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/92/5a33be64990ba815364a8f2dd9e6f51de60d23dfddafb4f1fc5577d4dc64/sentry_sdk-1.0.0-py2.py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 36.2MB/s \n",
            "\u001b[?25hCollecting pathtools\n",
            "  Downloading https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.1)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Collecting GitPython>=1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/99/98019716955ba243657daedd1de8f3a88ca1f5b75057c38e959db22fb87b/GitPython-3.1.14-py3-none-any.whl (159kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 38.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: Click>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Collecting configparser>=3.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/fd/01/ff260a18caaf4457eb028c96eeb405c4a230ca06c8ec9c1379f813caa52e/configparser-5.0.2-py3-none-any.whl\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.12.0->wandb) (54.0.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from sentry-sdk>=0.4.0->wandb) (2020.12.5)\n",
            "Requirement already satisfied: urllib3>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from sentry-sdk>=0.4.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/48/11/d1800bca0a3bae820b84b7d813ad1eff15a48a64caea9c823fc8c1b119e8/gitdb-4.0.5-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 6.9MB/s \n",
            "\u001b[?25hCollecting smmap<4,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/d5/1e/6130925131f639b2acde0f7f18b73e33ce082ff2d90783c436b52040af5a/smmap-3.0.5-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: subprocess32, pathtools\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-cp37-none-any.whl size=6489 sha256=4ff6f9fb3ad6b2bf69046111091bc38706d01676d9c23445271c5915390fdacc\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-cp37-none-any.whl size=8786 sha256=b8cb9c7532d7984af7895d8611f1f7a39e1680914b0b5d2d7dac3396082d6486\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843\n",
            "Successfully built subprocess32 pathtools\n",
            "Installing collected packages: subprocess32, shortuuid, sentry-sdk, pathtools, docker-pycreds, smmap, gitdb, GitPython, configparser, wandb\n",
            "Successfully installed GitPython-3.1.14 configparser-5.0.2 docker-pycreds-0.4.0 gitdb-4.0.5 pathtools-0.1.2 sentry-sdk-1.0.0 shortuuid-1.0.1 smmap-3.0.5 subprocess32-3.5.4 wandb-0.10.22\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vuMLV7jAhpo"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1.0 / (1 + np.exp(-(z)))\n",
        "\n",
        "\n",
        "def tanh(z):\n",
        "    return np.tanh(z)\n",
        "\n",
        "\n",
        "def sin(z):\n",
        "    return np.sin(z)\n",
        "\n",
        "\n",
        "def relu(z):\n",
        "    return (z>0)*(z) + ((z<0)*(z)*0.01)\n",
        "\n",
        "\n",
        "def softmax(Z):\n",
        "    return np.exp(Z) / np.sum(np.exp(Z))\n",
        "\n",
        "\n",
        "def der_sigmoid(z):\n",
        "    return  (1.0 / (1 + np.exp(-(z))))*(1 -  1.0 / (1 + np.exp(-(z))))\n",
        "\n",
        "def der_tanh(z):\n",
        "    return 1 - np.tanh(z) ** 2\n",
        "\n",
        "\n",
        "def der_relu(z):\n",
        "    return (z>0)*np.ones(z.shape) + (z<0)*(0.01*np.ones(z.shape) )"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Rcbw2UiAs_M"
      },
      "source": [
        "import numpy as np\n",
        "import scipy as sp\n",
        "import wandb\n",
        "import time\n",
        "\n",
        "import tensorflow.keras as tfkeras\n",
        "\n",
        "class FeedForwardNeuralNetwork:\n",
        "    def __init__(\n",
        "        self, \n",
        "        num_hidden_layers, \n",
        "        num_hidden_neurons, \n",
        "        X_train_raw, \n",
        "        Y_train_raw,  \n",
        "        N_train, \n",
        "        X_val_raw, \n",
        "        Y_val_raw, \n",
        "        N_val,\n",
        "        X_test_raw, \n",
        "        Y_test_raw, \n",
        "        N_test,        \n",
        "        optimizer,\n",
        "        batch_size,\n",
        "        weight_decay,\n",
        "        learning_rate,\n",
        "        max_epochs,\n",
        "        activation,\n",
        "        initializer,\n",
        "        loss\n",
        "\n",
        "    ):\n",
        "\n",
        "        \"\"\"\n",
        "        Here, we initialize the FeedForwardNeuralNetwork class with the number of hidden layers, number of hidden neurons, raw training data. \n",
        "        \"\"\"\n",
        "        \n",
        "        self.num_classes = np.max(Y_train_raw) + 1  # NUM_CLASSES\n",
        "        self.num_hidden_layers = num_hidden_layers\n",
        "        self.num_hidden_neurons = num_hidden_neurons\n",
        "        self.output_layer_size = self.num_classes\n",
        "        self.img_height = X_train_raw.shape[1]\n",
        "        self.img_width = X_train_raw.shape[2]\n",
        "        self.img_flattened_size = self.img_height * self.img_width\n",
        "\n",
        "        # self.layers = layers\n",
        "        self.layers = (\n",
        "            [self.img_flattened_size]\n",
        "            + num_hidden_layers * [num_hidden_neurons]\n",
        "            + [self.output_layer_size]\n",
        "        )\n",
        "\n",
        "        self.N_train = N_train\n",
        "        self.N_val = N_val\n",
        "        self.N_test = N_test\n",
        "        \n",
        "\n",
        "\n",
        "        self.X_train = np.transpose(\n",
        "            X_train_raw.reshape(\n",
        "                X_train_raw.shape[0], X_train_raw.shape[1] * X_train_raw.shape[2]\n",
        "            )\n",
        "        )  # [IMG_HEIGHT*IMG_WIDTH X NTRAIN]\n",
        "        self.X_test = np.transpose(\n",
        "            X_test_raw.reshape(\n",
        "                X_test_raw.shape[0], X_test_raw.shape[1] * X_test_raw.shape[2]\n",
        "            )\n",
        "        )  # [IMG_HEIGHT*IMG_WIDTH X NTRAIN]\n",
        "        self.X_val = np.transpose(\n",
        "            X_val_raw.reshape(\n",
        "                X_val_raw.shape[0], X_val_raw.shape[1] * X_val_raw.shape[2]\n",
        "            )\n",
        "        )  # [IMG_HEIGHT*IMG_WIDTH X NTRAIN]\n",
        "\n",
        "\n",
        "        self.X_train = self.X_train / 255\n",
        "        self.X_test = self.X_test / 255\n",
        "        self.X_val = self.X_val / 255\n",
        "        \n",
        "        self.Y_train = self.oneHotEncode(Y_train_raw)  # [NUM_CLASSES X NTRAIN]\n",
        "        self.Y_val = self.oneHotEncode(Y_val_raw)\n",
        "        self.Y_test = self.oneHotEncode(Y_test_raw)\n",
        "        #self.Y_shape = self.Y_train.shape\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # self.weights, self.biases = self.initializeNeuralNet(self.layers)\n",
        "\n",
        "\n",
        "\n",
        "        self.Activations_dict = {\"SIGMOID\": sigmoid, \"TANH\": tanh, \"RELU\": relu}\n",
        "        self.DerActivation_dict = {\n",
        "            \"SIGMOID\": der_sigmoid,\n",
        "            \"TANH\": der_tanh,\n",
        "            \"RELU\": der_relu,\n",
        "        }\n",
        "\n",
        "        self.Initializer_dict = {\n",
        "            \"XAVIER\": self.Xavier_initializer,\n",
        "            \"RANDOM\": self.random_initializer,\n",
        "            \"HE\": self.He_initializer\n",
        "        }\n",
        "\n",
        "        self.Optimizer_dict = {\n",
        "            \"SGD\": self.sgdMiniBatch,\n",
        "            \"MGD\": self.mgd,\n",
        "            \"NAG\": self.nag,\n",
        "            \"RMSPROP\": self.rmsProp,\n",
        "            \"ADAM\": self.adam,\n",
        "            \"NADAM\": self.nadam,\n",
        "        }\n",
        "        \n",
        "        self.activation = self.Activations_dict[activation]\n",
        "        self.der_activation = self.DerActivation_dict[activation]\n",
        "        self.optimizer = self.Optimizer_dict[optimizer]\n",
        "        self.initializer = self.Initializer_dict[initializer]\n",
        "        self.loss_function = loss\n",
        "        self.max_epochs = max_epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.learning_rate = learning_rate\n",
        "        \n",
        "        self.weights, self.biases = self.initializeNeuralNet(self.layers)\n",
        "\n",
        "\n",
        "        \n",
        "        \n",
        "    # helper functions\n",
        "    def oneHotEncode(self, Y_train_raw):\n",
        "        Ydata = np.zeros((self.num_classes, Y_train_raw.shape[0]))\n",
        "        for i in range(Y_train_raw.shape[0]):\n",
        "            value = Y_train_raw[i]\n",
        "            Ydata[int(value)][i] = 1.0\n",
        "        return Ydata\n",
        "\n",
        "    # Loss functions\n",
        "    def meanSquaredErrorLoss(self, Y_true, Y_pred):\n",
        "        MSE = np.mean((Y_true - Y_pred) ** 2)\n",
        "        return MSE\n",
        "\n",
        "    def crossEntropyLoss(self, Y_true, Y_pred):\n",
        "        CE = [-Y_true[i] * np.log(Y_pred[i]) for i in range(len(Y_pred))]\n",
        "        crossEntropy = np.mean(CE)\n",
        "        return crossEntropy\n",
        "\n",
        "    def L2RegularisationLoss(self, weight_decay):\n",
        "        ALPHA = weight_decay\n",
        "        return ALPHA * np.sum(\n",
        "            [\n",
        "                np.linalg.norm(self.weights[str(i + 1)]) ** 2\n",
        "                for i in range(len(self.weights))\n",
        "            ]\n",
        "        )\n",
        "\n",
        "\n",
        "    def accuracy(self, Y_true, Y_pred, data_size):\n",
        "        Y_true_label = []\n",
        "        Y_pred_label = []\n",
        "        ctr = 0\n",
        "        for i in range(data_size):\n",
        "            Y_true_label.append(np.argmax(Y_true[:, i]))\n",
        "            Y_pred_label.append(np.argmax(Y_pred[:, i]))\n",
        "            if Y_true_label[i] == Y_pred_label[i]:\n",
        "                ctr += 1\n",
        "        accuracy = ctr / data_size\n",
        "        return accuracy, Y_true_label, Y_pred_label\n",
        "\n",
        "    def Xavier_initializer(self, size):\n",
        "        in_dim = size[1]\n",
        "        out_dim = size[0]\n",
        "        xavier_stddev = np.sqrt(2 / (in_dim + out_dim))\n",
        "        return np.random.normal(0, xavier_stddev, size=(out_dim, in_dim))\n",
        "\n",
        "    #def Xavier_initializer(self, size):\n",
        "    #    in_dim = size[1]\n",
        "    #    out_dim = size[0]\n",
        "    #    #xavier_stddev = np.sqrt(2 / (in_dim + out_dim))\n",
        "    #    initializer = tfkeras.initializers.GlorotNormal()\n",
        "    #    return initializer(shape=(out_dim, in_dim)).numpy()\n",
        "\n",
        "    #def random_initializer(self, size):\n",
        "    #    in_dim = size[1]\n",
        "    #    out_dim = size[0]\n",
        "    #    #xavier_stddev = np.sqrt(2 / (in_dim + out_dim))\n",
        "    #    initializer = tfkeras.initializers.RandomNormal()\n",
        "    #    return initializer(shape=(out_dim, in_dim)).numpy()\n",
        "\n",
        "    def random_initializer(self, size):\n",
        "        in_dim = size[1]\n",
        "        out_dim = size[0]\n",
        "        return np.random.normal(0, 1, size=(out_dim, in_dim))\n",
        "\n",
        "\n",
        "    def He_initializer(self,size):\n",
        "        in_dim = size[1]\n",
        "        out_dim = size[0]\n",
        "        He_stddev = np.sqrt(2 / (in_dim))\n",
        "        return np.random.normal(0, 1, size=(out_dim, in_dim)) * He_stddev\n",
        "\n",
        "\n",
        "    def initializeNeuralNet(self, layers):\n",
        "        weights = {}\n",
        "        biases = {}\n",
        "        num_layers = len(layers)\n",
        "        for l in range(0, num_layers - 1):\n",
        "            W = self.initializer(size=[layers[l + 1], layers[l]])\n",
        "            b = np.zeros((layers[l + 1], 1))\n",
        "            weights[str(l + 1)] = W\n",
        "            biases[str(l + 1)] = b\n",
        "        return weights, biases\n",
        "\n",
        "    def forwardPropagate(self, X_train_batch, weights, biases):\n",
        "        \"\"\"\n",
        "        Returns the neural network given input data, weights, biases.\n",
        "        Arguments:\n",
        "                 : X - input matrix\n",
        "                 : Weights  - Weights matrix\n",
        "                 : biases - Bias vectors \n",
        "        \"\"\"\n",
        "        # Number of layers = length of weight matrix + 1\n",
        "        num_layers = len(weights) + 1\n",
        "        # A - Preactivations\n",
        "        # H - Activations\n",
        "        X = X_train_batch\n",
        "        H = {}\n",
        "        A = {}\n",
        "        H[\"0\"] = X\n",
        "        A[\"0\"] = X\n",
        "        for l in range(0, num_layers - 2):\n",
        "            if l == 0:\n",
        "                W = weights[str(l + 1)]\n",
        "                b = biases[str(l + 1)]\n",
        "                A[str(l + 1)] = np.add(np.matmul(W, X), b)\n",
        "                H[str(l + 1)] = self.activation(A[str(l + 1)])\n",
        "            else:\n",
        "                W = weights[str(l + 1)]\n",
        "                b = biases[str(l + 1)]\n",
        "                A[str(l + 1)] = np.add(np.matmul(W, H[str(l)]), b)\n",
        "                H[str(l + 1)] = self.activation(A[str(l + 1)])\n",
        "\n",
        "        # Here the last layer is not activated as it is a regression problem\n",
        "        W = weights[str(num_layers - 1)]\n",
        "        b = biases[str(num_layers - 1)]\n",
        "        A[str(num_layers - 1)] = np.add(np.matmul(W, H[str(num_layers - 2)]), b)\n",
        "        # Y = softmax(A[-1])\n",
        "        Y = softmax(A[str(num_layers - 1)])\n",
        "        H[str(num_layers - 1)] = Y\n",
        "        return Y, H, A\n",
        "\n",
        "    def backPropagate(\n",
        "        self, Y, H, A, Y_train_batch, weight_decay=0\n",
        "    ):\n",
        "\n",
        "        ALPHA = weight_decay\n",
        "        gradients_weights = []\n",
        "        gradients_biases = []\n",
        "        num_layers = len(self.layers)\n",
        "\n",
        "        # Gradient with respect to the output layer is absolutely fine.\n",
        "        if self.loss_function == \"CROSS\":\n",
        "            globals()[\"grad_a\" + str(num_layers - 1)] = -(Y_train_batch - Y)\n",
        "        elif self.loss_function == \"MSE\":\n",
        "            globals()[\"grad_a\" + str(num_layers - 1)] = np.multiply(\n",
        "                2 * (Y - Y_train_batch), np.multiply(Y, (1 - Y))\n",
        "            )\n",
        "\n",
        "        for l in range(num_layers - 2, -1, -1):\n",
        "\n",
        "            if ALPHA != 0:\n",
        "                globals()[\"grad_W\" + str(l + 1)] = (\n",
        "                    np.outer(globals()[\"grad_a\" + str(l + 1)], H[str(l)])\n",
        "                    + ALPHA * self.weights[str(l + 1)]\n",
        "                )\n",
        "            elif ALPHA == 0:\n",
        "                globals()[\"grad_W\" + str(l + 1)] = np.outer(\n",
        "                    globals()[\"grad_a\" + str(l + 1)], H[str(l)]\n",
        "                )\n",
        "            globals()[\"grad_b\" + str(l + 1)] = globals()[\"grad_a\" + str(l + 1)]\n",
        "            gradients_weights.append(globals()[\"grad_W\" + str(l + 1)])\n",
        "            gradients_biases.append(globals()[\"grad_b\" + str(l + 1)])\n",
        "            if l != 0:\n",
        "                globals()[\"grad_h\" + str(l)] = np.matmul(\n",
        "                    self.weights[str(l + 1)].transpose(),\n",
        "                    globals()[\"grad_a\" + str(l + 1)],\n",
        "                )\n",
        "                globals()[\"grad_a\" + str(l)] = np.multiply(\n",
        "                    globals()[\"grad_h\" + str(l)], self.der_activation(A[str(l)])\n",
        "                )\n",
        "            elif l == 0:\n",
        "\n",
        "                globals()[\"grad_h\" + str(l)] = np.matmul(\n",
        "                    self.weights[str(l + 1)].transpose(),\n",
        "                    globals()[\"grad_a\" + str(l + 1)],\n",
        "                )\n",
        "                globals()[\"grad_a\" + str(l)] = np.multiply(\n",
        "                    globals()[\"grad_h\" + str(l)], (A[str(l)])\n",
        "                )\n",
        "        return gradients_weights, gradients_biases\n",
        "\n",
        "\n",
        "    def predict(self,X,length_dataset):\n",
        "        Y_pred = []        \n",
        "        for i in range(length_dataset):\n",
        "\n",
        "            Y, H, A = self.forwardPropagate(\n",
        "                X[:, i].reshape(self.img_flattened_size, 1),\n",
        "                self.weights,\n",
        "                self.biases,\n",
        "            )\n",
        "\n",
        "            Y_pred.append(Y.reshape(self.num_classes,))\n",
        "        Y_pred = np.array(Y_pred).transpose()\n",
        "        return Y_pred\n",
        "\n",
        "    def sgd(self, epochs, length_dataset, learning_rate, weight_decay=0):\n",
        "        \n",
        "        trainingloss = []\n",
        "        trainingaccuracy = []\n",
        "        validationaccuracy = []\n",
        "        \n",
        "        num_layers = len(self.layers)\n",
        "\n",
        "        X_train = self.X_train[:, :length_dataset]\n",
        "        Y_train = self.Y_train[:, :length_dataset]\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            start_time = time.time()\n",
        "            # perm = np.random.permutation(N)\n",
        "            idx = np.random.shuffle(np.arange(length_dataset))\n",
        "            X_train = X_train[:, idx].reshape(self.img_flattened_size, length_dataset)\n",
        "            Y_train = Y_train[:, idx].reshape(self.num_classes, length_dataset)\n",
        "\n",
        "            CE = []\n",
        "            #Y_pred = []\n",
        "            deltaw = [\n",
        "                np.zeros((self.layers[l + 1], self.layers[l]))\n",
        "                for l in range(0, len(self.layers) - 1)\n",
        "            ]\n",
        "            deltab = [\n",
        "                np.zeros((self.layers[l + 1], 1))\n",
        "                for l in range(0, len(self.layers) - 1)\n",
        "            ]\n",
        "\n",
        "            for i in range(length_dataset):\n",
        "\n",
        "                Y, H, A = self.forwardPropagate(\n",
        "                    X_train[:, i].reshape(self.img_flattened_size, 1),\n",
        "                    self.weights,\n",
        "                    self.biases,\n",
        "                )\n",
        "                grad_weights, grad_biases = self.backPropagate(\n",
        "                    Y, H, A, Y_train[:, i].reshape(self.num_classes, 1)\n",
        "                )\n",
        "                deltaw = [\n",
        "                    grad_weights[num_layers - 2 - i] for i in range(num_layers - 1)\n",
        "                ]\n",
        "                deltab = [\n",
        "                    grad_biases[num_layers - 2 - i] for i in range(num_layers - 1)\n",
        "                ]\n",
        "\n",
        "                #Y_pred.append(Y.reshape(self.num_classes,))\n",
        "\n",
        "                CE.append(\n",
        "                    self.crossEntropyLoss(\n",
        "                        self.Y_train[:, i].reshape(self.num_classes, 1), Y\n",
        "                    )\n",
        "                    + self.L2RegularisationLoss(weight_decay)\n",
        "                )\n",
        "\n",
        "                # print(num_points_seen)\n",
        "                self.weights = {\n",
        "                    str(i + 1): (self.weights[str(i + 1)] - learning_rate * deltaw[i])\n",
        "                    for i in range(len(self.weights))\n",
        "                }\n",
        "                self.biases = {\n",
        "                    str(i + 1): (self.biases[str(i + 1)] - learning_rate * deltab[i])\n",
        "                    for i in range(len(self.biases))\n",
        "                }\n",
        "\n",
        "            elapsed = time.time() - start_time\n",
        "            #Y_pred = np.array(Y_pred).transpose()\n",
        "            Y_pred = self.predict(self.X_train, self.N_train)\n",
        "            trainingloss.append(np.mean(CE))\n",
        "            trainingaccuracy.append(self.accuracy(Y_train, Y_pred, length_dataset)[0])\n",
        "            validationaccuracy.append(self.accuracy(self.Y_val, self.predict(self.X_val, self.N_val), self.N_val)[0])\n",
        "            \n",
        "            print(\n",
        "                        \"Epoch: %d, Loss: %.3e, Training accuracy:%.2f, Validation Accuracy: %.2f, Time: %.2f, Learning Rate: %.3e\"\n",
        "                        % (\n",
        "                            epoch,\n",
        "                            trainingloss[epoch],\n",
        "                            trainingaccuracy[epoch],\n",
        "                            validationaccuracy[epoch],\n",
        "                            elapsed,\n",
        "                            self.learning_rate,\n",
        "                        )\n",
        "                    )\n",
        "\n",
        "            wandb.log({'loss':np.mean(CE), 'trainingaccuracy':trainingaccuracy[epoch], 'validationaccuracy':validationaccuracy[epoch],'epoch':epoch, })\n",
        "        # data = [[epoch, loss[epoch]] for epoch in range(epochs)]\n",
        "        # table = wandb.Table(data=data, columns = [\"Epoch\", \"Loss\"])\n",
        "        # wandb.log({'loss':wandb.plot.line(table, \"Epoch\", \"Loss\", title=\"Loss vs Epoch Line Plot\")})\n",
        "        return trainingloss, trainingaccuracy, validationaccuracy, Y_pred\n",
        "\n",
        "\n",
        "      \n",
        "    def sgdMiniBatch(self, epochs,length_dataset, batch_size, learning_rate, weight_decay = 0):\n",
        "\n",
        "        X_train = self.X_train[:, :length_dataset]\n",
        "        Y_train = self.Y_train[:, :length_dataset]        \n",
        "\n",
        "        trainingloss = []\n",
        "        trainingaccuracy = []\n",
        "        validationaccuracy = []\n",
        "        \n",
        "        num_layers = len(self.layers)\n",
        "        num_points_seen = 0\n",
        "\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            start_time = time.time()\n",
        "            idx = np.random.shuffle(np.arange(length_dataset))\n",
        "            X_train = X_train[:, idx].reshape(self.img_flattened_size, length_dataset)\n",
        "            Y_train = Y_train[:, idx].reshape(self.num_classes, length_dataset)\n",
        "            \n",
        "            CE = []\n",
        "            #Y_pred = []\n",
        "            \n",
        "            deltaw = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "            deltab = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "\n",
        "            for i in range(length_dataset):\n",
        "                \n",
        "                Y,H,A = self.forwardPropagate(X_train[:,i].reshape(self.img_flattened_size,1), self.weights, self.biases) \n",
        "                grad_weights, grad_biases = self.backPropagate(Y,H,A,Y_train[:,i].reshape(self.num_classes,1))\n",
        "                \n",
        "                deltaw = [grad_weights[num_layers-2 - i] + deltaw[i] for i in range(num_layers - 1)]\n",
        "                deltab = [grad_biases[num_layers-2 - i] + deltab[i] for i in range(num_layers - 1)]\n",
        "                \n",
        "                #Y_pred.append(Y.reshape(self.num_classes,))\n",
        "                CE.append(self.crossEntropyLoss(self.Y_train[:,i].reshape(self.num_classes,1), Y) + self.L2RegularisationLoss(weight_decay))\n",
        "                \n",
        "                num_points_seen +=1\n",
        "                \n",
        "                if int(num_points_seen) % batch_size == 0:\n",
        "                    \n",
        "                    \n",
        "                    self.weights = {str(i+1):(self.weights[str(i+1)] - learning_rate*deltaw[i]/batch_size) for i in range(len(self.weights))} \n",
        "                    self.biases = {str(i+1):(self.biases[str(i+1)] - learning_rate*deltab[i]) for i in range(len(self.biases))}\n",
        "                    \n",
        "                    #resetting gradient updates\n",
        "                    deltaw = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "                    deltab = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "            \n",
        "            elapsed = time.time() - start_time\n",
        "            #Y_pred = np.array(Y_pred).transpose()\n",
        "            Y_pred = self.predict(self.X_train, self.N_train)\n",
        "            trainingloss.append(np.mean(CE))\n",
        "            trainingaccuracy.append(self.accuracy(Y_train, Y_pred, length_dataset)[0])\n",
        "            validationaccuracy.append(self.accuracy(self.Y_val, self.predict(self.X_val, self.N_val), self.N_val)[0])\n",
        "\n",
        "            print(\n",
        "                        \"Epoch: %d, Loss: %.3e, Training accuracy:%.2f, Validation Accuracy: %.2f, Time: %.2f, Learning Rate: %.3e\"\n",
        "                        % (\n",
        "                            epoch,\n",
        "                            trainingloss[epoch],\n",
        "                            trainingaccuracy[epoch],\n",
        "                            validationaccuracy[epoch],\n",
        "                            elapsed,\n",
        "                            self.learning_rate,\n",
        "                        )\n",
        "                    )\n",
        "                    \n",
        "            wandb.log({'loss':np.mean(CE), 'trainingaccuracy':trainingaccuracy[epoch], 'validationaccuracy':validationaccuracy[epoch],'epoch':epoch })\n",
        "            \n",
        "        return trainingloss, trainingaccuracy, validationaccuracy, Y_pred\n",
        "\n",
        "\n",
        "\n",
        "    def mgd(self, epochs,length_dataset, batch_size, learning_rate, weight_decay = 0):\n",
        "        GAMMA = 0.9\n",
        "\n",
        "        X_train = self.X_train[:, :length_dataset]\n",
        "        Y_train = self.Y_train[:, :length_dataset]        \n",
        "\n",
        "        \n",
        "        trainingloss = []\n",
        "        trainingaccuracy = []\n",
        "        validationaccuracy = []\n",
        "        \n",
        "        num_layers = len(self.layers)\n",
        "        prev_v_w = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "        prev_v_b = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "        num_points_seen = 0\n",
        "        for epoch in range(epochs):\n",
        "            start_time = time.time()\n",
        "            idx = np.random.shuffle(np.arange(length_dataset))\n",
        "            X_train = X_train[:, idx].reshape(self.img_flattened_size, length_dataset)\n",
        "            Y_train = Y_train[:, idx].reshape(self.num_classes, length_dataset)\n",
        "\n",
        "            CE = []\n",
        "            #Y_pred = []\n",
        "            deltaw = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "            deltab = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "            \n",
        "\n",
        "            for i in range(length_dataset):\n",
        "                Y,H,A = self.forwardPropagate(self.X_train[:,i].reshape(self.img_flattened_size,1), self.weights, self.biases) \n",
        "                grad_weights, grad_biases = self.backPropagate(Y,H,A,self.Y_train[:,i].reshape(self.num_classes,1))\n",
        "                \n",
        "                deltaw = [grad_weights[num_layers-2 - i] + deltaw[i] for i in range(num_layers - 1)]\n",
        "                deltab = [grad_biases[num_layers-2 - i] + deltab[i] for i in range(num_layers - 1)]\n",
        "\n",
        "                #Y_pred.append(Y.reshape(self.num_classes,))\n",
        "                CE.append(self.crossEntropyLoss(self.Y_train[:,i].reshape(self.num_classes,1), Y) + self.L2RegularisationLoss(weight_decay))\n",
        "                \n",
        "                num_points_seen +=1\n",
        "                \n",
        "                if int(num_points_seen) % batch_size == 0:\n",
        "\n",
        "                    v_w = [GAMMA*prev_v_w[i] + learning_rate*deltaw[i]/batch_size for i in range(num_layers - 1)]\n",
        "                    v_b = [GAMMA*prev_v_b[i] + learning_rate*deltab[i]/batch_size for i in range(num_layers - 1)]\n",
        "                    \n",
        "                    self.weights = {str(i+1) : (self.weights[str(i+1)] - v_w[i]) for i in range(len(self.weights))}\n",
        "                    self.biases = {str(i+1): (self.biases[str(i+1)] - v_b[i]) for i in range(len(self.biases))}\n",
        "\n",
        "                    prev_v_w = v_w\n",
        "                    prev_v_b = v_b\n",
        "\n",
        "                    #resetting gradient updates\n",
        "                    deltaw = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "                    deltab = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "\n",
        "            elapsed = time.time() - start_time\n",
        "            #Y_pred = np.array(Y_pred).transpose()\n",
        "            Y_pred = self.predict(self.X_train, self.N_train)\n",
        "            trainingloss.append(np.mean(CE))\n",
        "            trainingaccuracy.append(self.accuracy(Y_train, Y_pred, length_dataset)[0])\n",
        "            validationaccuracy.append(self.accuracy(self.Y_val, self.predict(self.X_val, self.N_val), self.N_val)[0])\n",
        "\n",
        "            print(\n",
        "                        \"Epoch: %d, Loss: %.3e, Training accuracy:%.2f, Validation Accuracy: %.2f, Time: %.2f, Learning Rate: %.3e\"\n",
        "                        % (\n",
        "                            epoch,\n",
        "                            trainingloss[epoch],\n",
        "                            trainingaccuracy[epoch],\n",
        "                            validationaccuracy[epoch],\n",
        "                            elapsed,\n",
        "                            self.learning_rate,\n",
        "                        )\n",
        "                    )\n",
        "\n",
        "            wandb.log({'loss':np.mean(CE), 'trainingaccuracy':trainingaccuracy[epoch], 'validationaccuracy':validationaccuracy[epoch],'epoch':epoch })\n",
        "\n",
        "\n",
        "        return trainingloss, trainingaccuracy, validationaccuracy, Y_pred\n",
        "\n",
        "\n",
        " \n",
        " \n",
        "    def stochasticNag(self,epochs,length_dataset, learning_rate, weight_decay = 0):\n",
        "        GAMMA = 0.9\n",
        "\n",
        "        X_train = self.X_train[:, :length_dataset]\n",
        "        Y_train = self.Y_train[:, :length_dataset]        \n",
        "\n",
        "        trainingloss = []\n",
        "        trainingaccuracy = []\n",
        "        validationaccuracy = []\n",
        "        \n",
        "        num_layers = len(self.layers)\n",
        "        \n",
        "        prev_v_w = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "        prev_v_b = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "        \n",
        "        for epoch in range(epochs):\n",
        "            start_time = time.time()\n",
        "            idx = np.random.shuffle(np.arange(length_dataset))\n",
        "            X_train = X_train[:, idx].reshape(self.img_flattened_size, length_dataset)\n",
        "            Y_train = Y_train[:, idx].reshape(self.num_classes, length_dataset)\n",
        "\n",
        "            CE = []\n",
        "            #Y_pred = []  \n",
        "            \n",
        "            deltaw = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "            deltab = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "            \n",
        "            v_w = [GAMMA*prev_v_w[i] for i in range(0, len(self.layers)-1)]  \n",
        "            v_b = [GAMMA*prev_v_b[i] for i in range(0, len(self.layers)-1)]\n",
        "                        \n",
        "            for i in range(length_dataset):\n",
        "                winter = {str(i+1) : self.weights[str(i+1)] - v_w[i] for i in range(0, len(self.layers)-1)}\n",
        "                binter = {str(i+1) : self.biases[str(i+1)] - v_b[i] for i in range(0, len(self.layers)-1)}\n",
        "                \n",
        "                Y,H,A = self.forwardPropagate(self.X_train[:,i].reshape(self.img_flattened_size,1), winter, binter) \n",
        "                grad_weights, grad_biases = self.backPropagate(Y,H,A,self.Y_train[:,i].reshape(self.num_classes,1))\n",
        "                \n",
        "                deltaw = [grad_weights[num_layers-2 - i] for i in range(num_layers - 1)]\n",
        "                deltab = [grad_biases[num_layers-2 - i] for i in range(num_layers - 1)]\n",
        "\n",
        "                #Y_pred.append(Y.reshape(self.num_classes,))\n",
        "                CE.append(self.crossEntropyLoss(self.Y_train[:,i].reshape(self.num_classes,1), Y) + self.L2RegularisationLoss(weight_decay))\n",
        "                            \n",
        "                v_w = [GAMMA*prev_v_w[i] + learning_rate*deltaw[i] for i in range(num_layers - 1)]\n",
        "                v_b = [GAMMA*prev_v_b[i] + learning_rate*deltab[i] for i in range(num_layers - 1)]\n",
        "        \n",
        "                self.weights = {str(i+1):self.weights[str(i+1)] - v_w[i] for i in range(len(self.weights))} \n",
        "                self.biases = {str(i+1):self.biases[str(i+1)] - v_b[i] for i in range(len(self.biases))}\n",
        "                \n",
        "                prev_v_w = v_w\n",
        "                prev_v_b = v_b\n",
        "    \n",
        "            \n",
        "            elapsed = time.time() - start_time\n",
        "            #Y_pred = np.array(Y_pred).transpose()\n",
        "            Y_pred = self.predict(self.X_train, self.N_train)\n",
        "            trainingloss.append(np.mean(CE))\n",
        "            trainingaccuracy.append(self.accuracy(Y_train, Y_pred, length_dataset)[0])\n",
        "            validationaccuracy.append(self.accuracy(self.Y_val, self.predict(self.X_val, self.N_val), self.N_val)[0])\n",
        "\n",
        "            print(\n",
        "                        \"Epoch: %d, Loss: %.3e, Training accuracy:%.2f, Validation Accuracy: %.2f, Time: %.2f, Learning Rate: %.3e\"\n",
        "                        % (\n",
        "                            epoch,\n",
        "                            trainingloss[epoch],\n",
        "                            trainingaccuracy[epoch],\n",
        "                            validationaccuracy[epoch],\n",
        "                            elapsed,\n",
        "                            self.learning_rate,\n",
        "                        )\n",
        "                    )\n",
        "                    \n",
        "            wandb.log({'loss':np.mean(CE), 'trainingaccuracy':trainingaccuracy[epoch], 'validationaccuracy':validationaccuracy[epoch],'epoch':epoch })\n",
        "        \n",
        "        return trainingloss, trainingaccuracy, validationaccuracy, Y_pred\n",
        "    \n",
        "\n",
        "    def nag(self,epochs,length_dataset, batch_size,learning_rate, weight_decay = 0):\n",
        "        GAMMA = 0.9\n",
        "\n",
        "        X_train = self.X_train[:, :length_dataset]\n",
        "        Y_train = self.Y_train[:, :length_dataset]        \n",
        "\n",
        "\n",
        "        trainingloss = []\n",
        "        trainingaccuracy = []\n",
        "        validationaccuracy = []\n",
        "        \n",
        "        num_layers = len(self.layers)\n",
        "        \n",
        "        prev_v_w = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "        prev_v_b = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "        \n",
        "        num_points_seen = 0\n",
        "        for epoch in range(epochs):\n",
        "            start_time = time.time()\n",
        "            idx = np.random.shuffle(np.arange(length_dataset))\n",
        "            X_train = X_train[:, idx].reshape(self.img_flattened_size, length_dataset)\n",
        "            Y_train = Y_train[:, idx].reshape(self.num_classes, length_dataset)\n",
        "\n",
        "            CE = []\n",
        "            #Y_pred = []  \n",
        "            \n",
        "            deltaw = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "            deltab = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "            \n",
        "            v_w = [GAMMA*prev_v_w[i] for i in range(0, len(self.layers)-1)]  \n",
        "            v_b = [GAMMA*prev_v_b[i] for i in range(0, len(self.layers)-1)]\n",
        "\n",
        "            for i in range(length_dataset):\n",
        "                winter = {str(i+1) : self.weights[str(i+1)] - v_w[i] for i in range(0, len(self.layers)-1)}\n",
        "                binter = {str(i+1) : self.biases[str(i+1)] - v_b[i] for i in range(0, len(self.layers)-1)}\n",
        "                \n",
        "                Y,H,A = self.forwardPropagate(self.X_train[:,i].reshape(self.img_flattened_size,1), winter, binter) \n",
        "                grad_weights, grad_biases = self.backPropagate(Y,H,A,self.Y_train[:,i].reshape(self.num_classes,1))\n",
        "                \n",
        "                deltaw = [grad_weights[num_layers-2 - i] + deltaw[i] for i in range(num_layers - 1)]\n",
        "                deltab = [grad_biases[num_layers-2 - i] + deltab[i] for i in range(num_layers - 1)]\n",
        "\n",
        "                #Y_pred.append(Y.reshape(self.num_classes,))\n",
        "                CE.append(self.crossEntropyLoss(self.Y_train[:,i].reshape(self.num_classes,1), Y) + self.L2RegularisationLoss(weight_decay))\n",
        "\n",
        "                num_points_seen +=1\n",
        "                \n",
        "                if int(num_points_seen) % batch_size == 0:                            \n",
        "\n",
        "                    v_w = [GAMMA*prev_v_w[i] + learning_rate*deltaw[i]/batch_size for i in range(num_layers - 1)]\n",
        "                    v_b = [GAMMA*prev_v_b[i] + learning_rate*deltab[i]/batch_size for i in range(num_layers - 1)]\n",
        "        \n",
        "                    self.weights ={str(i+1):self.weights[str(i+1)]  - v_w[i] for i in range(len(self.weights))}\n",
        "                    self.biases = {str(i+1):self.biases[str(i+1)]  - v_b[i] for i in range(len(self.biases))}\n",
        "                \n",
        "                    prev_v_w = v_w\n",
        "                    prev_v_b = v_b\n",
        "\n",
        "                    deltaw = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "                    deltab = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "\n",
        "    \n",
        "            \n",
        "            elapsed = time.time() - start_time\n",
        "            #Y_pred = np.array(Y_pred).transpose()\n",
        "            Y_pred = self.predict(self.X_train, self.N_train)\n",
        "            trainingloss.append(np.mean(CE))\n",
        "            trainingaccuracy.append(self.accuracy(Y_train, Y_pred, length_dataset)[0])\n",
        "            validationaccuracy.append(self.accuracy(self.Y_val, self.predict(self.X_val, self.N_val), self.N_val)[0])\n",
        "\n",
        "            print(\n",
        "                        \"Epoch: %d, Loss: %.3e, Training accuracy:%.2f, Validation Accuracy: %.2f, Time: %.2f, Learning Rate: %.3e\"\n",
        "                        % (\n",
        "                            epoch,\n",
        "                            trainingloss[epoch],\n",
        "                            trainingaccuracy[epoch],\n",
        "                            validationaccuracy[epoch],\n",
        "                            elapsed,\n",
        "                            self.learning_rate,\n",
        "                        )\n",
        "                    )\n",
        "\n",
        "            wandb.log({'loss':np.mean(CE), 'trainingaccuracy':trainingaccuracy[epoch], 'validationaccuracy':validationaccuracy[epoch],'epoch':epoch })\n",
        "        \n",
        "        return trainingloss, trainingaccuracy, validationaccuracy, Y_pred\n",
        "    \n",
        "\n",
        "    \n",
        "    def rmsProp(self, epochs,length_dataset, batch_size, learning_rate, weight_decay = 0):\n",
        "\n",
        "\n",
        "        X_train = self.X_train[:, :length_dataset]\n",
        "        Y_train = self.Y_train[:, :length_dataset]        \n",
        "\n",
        "        \n",
        "        trainingloss = []\n",
        "        trainingaccuracy = []\n",
        "        validationaccuracy = []\n",
        "        \n",
        "        num_layers = len(self.layers)\n",
        "        EPS, BETA = 1e-8, 0.9\n",
        "        \n",
        "        v_w = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "        v_b = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "        \n",
        "        num_points_seen = 0        \n",
        "        for epoch in range(epochs):\n",
        "            start_time = time.time()\n",
        "            idx = np.random.shuffle(np.arange(length_dataset))\n",
        "            X_train = X_train[:, idx].reshape(self.img_flattened_size, length_dataset)\n",
        "            Y_train = Y_train[:, idx].reshape(self.num_classes, length_dataset)\n",
        "\n",
        "\n",
        "            CE = []\n",
        "            #Y_pred = []\n",
        "                        \n",
        "            deltaw = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "            deltab = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "\n",
        "            for i in range(length_dataset):\n",
        "            \n",
        "                Y,H,A = self.forwardPropagate(self.X_train[:,i].reshape(self.img_flattened_size,1), self.weights, self.biases) \n",
        "                grad_weights, grad_biases = self.backPropagate(Y,H,A,self.Y_train[:,i].reshape(self.num_classes,1))\n",
        "            \n",
        "                deltaw = [grad_weights[num_layers-2 - i] + deltaw[i] for i in range(num_layers - 1)]\n",
        "                deltab = [grad_biases[num_layers-2 - i] + deltab[i] for i in range(num_layers - 1)]\n",
        "                \n",
        "                #Y_pred.append(Y.reshape(self.num_classes,))\n",
        "                CE.append(self.crossEntropyLoss(self.Y_train[:,i].reshape(self.num_classes,1), Y) + self.L2RegularisationLoss(weight_decay))            \n",
        "                num_points_seen +=1\n",
        "                \n",
        "                if int(num_points_seen) % batch_size == 0:\n",
        "                \n",
        "                    v_w = [BETA*v_w[i] + (1-BETA)*(deltaw[i])**2 for i in range(num_layers - 1)]\n",
        "                    v_b = [BETA*v_b[i] + (1-BETA)*(deltab[i])**2 for i in range(num_layers - 1)]\n",
        "\n",
        "                    self.weights = {str(i+1):self.weights[str(i+1)]  - deltaw[i]*(learning_rate/np.sqrt(v_w[i]+EPS)) for i in range(len(self.weights))} \n",
        "                    self.biases = {str(i+1):self.biases[str(i+1)]  - deltab[i]*(learning_rate/np.sqrt(v_b[i]+EPS)) for i in range(len(self.biases))}\n",
        "\n",
        "                    deltaw = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "                    deltab = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "    \n",
        "            \n",
        "            elapsed = time.time() - start_time\n",
        "            #Y_pred = np.array(Y_pred).transpose()\n",
        "            Y_pred = self.predict(self.X_train, self.N_train)\n",
        "            trainingloss.append(np.mean(CE))\n",
        "            trainingaccuracy.append(self.accuracy(Y_train, Y_pred, length_dataset)[0])\n",
        "            validationaccuracy.append(self.accuracy(self.Y_val, self.predict(self.X_val, self.N_val), self.N_val)[0])\n",
        "\n",
        "            print(\n",
        "                        \"Epoch: %d, Loss: %.3e, Training accuracy:%.2f, Validation Accuracy: %.2f, Time: %.2f, Learning Rate: %.3e\"\n",
        "                        % (\n",
        "                            epoch,\n",
        "                            trainingloss[epoch],\n",
        "                            trainingaccuracy[epoch],\n",
        "                            validationaccuracy[epoch],\n",
        "                            elapsed,\n",
        "                            self.learning_rate,\n",
        "                        )\n",
        "                    )\n",
        "                    \n",
        "            wandb.log({'loss':np.mean(CE), 'trainingaccuracy':trainingaccuracy[epoch], 'validationaccuracy':validationaccuracy[epoch],'epoch':epoch })\n",
        "        \n",
        "        return trainingloss, trainingaccuracy, validationaccuracy, Y_pred  \n",
        "\n",
        "\n",
        "\n",
        "    def adam(self, epochs,length_dataset, batch_size, learning_rate, weight_decay = 0):\n",
        "        \n",
        "        X_train = self.X_train[:, :length_dataset]\n",
        "        Y_train = self.Y_train[:, :length_dataset]        \n",
        "\n",
        "        trainingloss = []\n",
        "        trainingaccuracy = []\n",
        "        validationaccuracy = []\n",
        "        num_layers = len(self.layers)\n",
        "        EPS, BETA1, BETA2 = 1e-8, 0.9, 0.99\n",
        "        \n",
        "        m_w = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "        m_b = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "        \n",
        "        v_w = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "        v_b = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]        \n",
        "        \n",
        "        m_w_hat = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "        m_b_hat = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "        \n",
        "        v_w_hat = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "        v_b_hat = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]   \n",
        "        \n",
        "        num_points_seen = 0 \n",
        "        for epoch in range(epochs):\n",
        "            start_time = time.time()\n",
        "            idx = np.random.shuffle(np.arange(length_dataset))\n",
        "            X_train = X_train[:, idx].reshape(self.img_flattened_size, length_dataset)\n",
        "            Y_train = Y_train[:, idx].reshape(self.num_classes, length_dataset)\n",
        "\n",
        "\n",
        "            CE = []\n",
        "            #Y_pred = []\n",
        "            \n",
        "            deltaw = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "            deltab = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "            \n",
        "           \n",
        "            for i in range(length_dataset):\n",
        "                Y,H,A = self.forwardPropagate(self.X_train[:,i].reshape(self.img_flattened_size,1), self.weights, self.biases) \n",
        "                grad_weights, grad_biases = self.backPropagate(Y,H,A,self.Y_train[:,i].reshape(self.num_classes,1))\n",
        "                \n",
        "                deltaw = [grad_weights[num_layers-2 - i] + deltaw[i] for i in range(num_layers - 1)]\n",
        "                deltab = [grad_biases[num_layers-2 - i] + deltab[i] for i in range(num_layers - 1)]\n",
        "\n",
        "                #Y_pred.append(Y.reshape(self.num_classes,))\n",
        "                CE.append(self.crossEntropyLoss(self.Y_train[:,i].reshape(self.num_classes,1), Y) + self.L2RegularisationLoss(weight_decay))                 \n",
        "\n",
        "                num_points_seen += 1\n",
        "                ctr = 0\n",
        "                if int(num_points_seen) % batch_size == 0:\n",
        "                    ctr += 1\n",
        "                \n",
        "                    m_w = [BETA1*m_w[i] + (1-BETA1)*deltaw[i] for i in range(num_layers - 1)]\n",
        "                    m_b = [BETA1*m_b[i] + (1-BETA1)*deltab[i] for i in range(num_layers - 1)]\n",
        "                \n",
        "                    v_w = [BETA2*v_w[i] + (1-BETA2)*(deltaw[i])**2 for i in range(num_layers - 1)]\n",
        "                    v_b = [BETA2*v_b[i] + (1-BETA2)*(deltab[i])**2 for i in range(num_layers - 1)]\n",
        "                    \n",
        "                    m_w_hat = [m_w[i]/(1-BETA1**(epoch+1)) for i in range(num_layers - 1)]\n",
        "                    m_b_hat = [m_b[i]/(1-BETA1**(epoch+1)) for i in range(num_layers - 1)]            \n",
        "                \n",
        "                    v_w_hat = [v_w[i]/(1-BETA2**(epoch+1)) for i in range(num_layers - 1)]\n",
        "                    v_b_hat = [v_b[i]/(1-BETA2**(epoch+1)) for i in range(num_layers - 1)]\n",
        "                \n",
        "                    self.weights = {str(i+1):self.weights[str(i+1)] - (learning_rate/np.sqrt(v_w[i]+EPS))*m_w_hat[i] for i in range(len(self.weights))} \n",
        "                    self.biases = {str(i+1):self.biases[str(i+1)] - (learning_rate/np.sqrt(v_b[i]+EPS))*m_b_hat[i] for i in range(len(self.biases))}\n",
        "\n",
        "                    deltaw = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "                    deltab = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "\n",
        "\n",
        "            elapsed = time.time() - start_time\n",
        "            #Y_pred = np.array(Y_pred).transpose()\n",
        "            Y_pred = self.predict(self.X_train, self.N_train)\n",
        "            trainingloss.append(np.mean(CE))\n",
        "            trainingaccuracy.append(self.accuracy(Y_train, Y_pred, length_dataset)[0])\n",
        "            validationaccuracy.append(self.accuracy(self.Y_val, self.predict(self.X_val, self.N_val), self.N_val)[0])\n",
        "\n",
        "            print(\n",
        "                        \"Epoch: %d, Loss: %.3e, Training accuracy:%.2f, Validation Accuracy: %.2f, Time: %.2f, Learning Rate: %.3e\"\n",
        "                        % (\n",
        "                            epoch,\n",
        "                            trainingloss[epoch],\n",
        "                            trainingaccuracy[epoch],\n",
        "                            validationaccuracy[epoch],\n",
        "                            elapsed,\n",
        "                            self.learning_rate,\n",
        "                        )\n",
        "                    )\n",
        "                    \n",
        "            wandb.log({'loss':np.mean(CE), 'trainingaccuracy':trainingaccuracy[epoch], 'validationaccuracy':validationaccuracy[epoch],'epoch':epoch })\n",
        "        \n",
        "        return trainingloss, trainingaccuracy, validationaccuracy, Y_pred\n",
        "\n",
        "\n",
        "    \n",
        "    def nadam(self, epochs,length_dataset, batch_size, learning_rate, weight_decay = 0):\n",
        "\n",
        "        X_train = self.X_train[:, :length_dataset]\n",
        "        Y_train = self.Y_train[:, :length_dataset]        \n",
        "\n",
        "        \n",
        "        trainingloss = []\n",
        "        trainingaccuracy = []\n",
        "        validationaccuracy = []\n",
        "        num_layers = len(self.layers)\n",
        "        \n",
        "        GAMMA, EPS, BETA1, BETA2 = 0.9, 1e-8, 0.9, 0.99\n",
        "\n",
        "        m_w = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "        m_b = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "        \n",
        "        v_w = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "        v_b = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]        \n",
        "\n",
        "        m_w_hat = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "        m_b_hat = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "        \n",
        "        v_w_hat = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "        v_b_hat = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)] \n",
        "\n",
        "        num_points_seen = 0 \n",
        "        \n",
        "        \n",
        "        for epoch in range(epochs):\n",
        "            start_time = time.time()\n",
        "            idx = np.random.shuffle(np.arange(length_dataset))\n",
        "            X_train = X_train[:, idx].reshape(self.img_flattened_size, length_dataset)\n",
        "            Y_train = Y_train[:, idx].reshape(self.num_classes, length_dataset)\n",
        "\n",
        "            CE = []\n",
        "            #Y_pred = []\n",
        "\n",
        "            deltaw = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "            deltab = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "\n",
        "            for i in range(length_dataset):\n",
        "\n",
        "                Y,H,A = self.forwardPropagate(self.X_train[:,i].reshape(self.img_flattened_size,1), self.weights, self.biases) \n",
        "                grad_weights, grad_biases = self.backPropagate(Y,H,A,self.Y_train[:,i].reshape(self.num_classes,1))\n",
        "\n",
        "                deltaw = [grad_weights[num_layers-2 - i] + deltaw[i] for i in range(num_layers - 1)]\n",
        "                deltab = [grad_biases[num_layers-2 - i] + deltab[i] for i in range(num_layers - 1)]\n",
        "\n",
        "                #Y_pred.append(Y.reshape(self.num_classes,))\n",
        "                CE.append(self.crossEntropyLoss(self.Y_train[:,i].reshape(self.num_classes,1), Y) + self.L2RegularisationLoss(weight_decay))   \n",
        "                num_points_seen += 1\n",
        "                \n",
        "                if num_points_seen % batch_size == 0:\n",
        "                    \n",
        "                    m_w = [BETA1*m_w[i] + (1-BETA1)*deltaw[i] for i in range(num_layers - 1)]\n",
        "                    m_b = [BETA1*m_b[i] + (1-BETA1)*deltab[i] for i in range(num_layers - 1)]\n",
        "                    \n",
        "                    v_w = [BETA2*v_w[i] + (1-BETA2)*(deltaw[i])**2 for i in range(num_layers - 1)]\n",
        "                    v_b = [BETA2*v_b[i] + (1-BETA2)*(deltab[i])**2 for i in range(num_layers - 1)]\n",
        "                    \n",
        "                    m_w_hat = [m_w[i]/(1-BETA1**(epoch+1)) for i in range(num_layers - 1)]\n",
        "                    m_b_hat = [m_b[i]/(1-BETA1**(epoch+1)) for i in range(num_layers - 1)]            \n",
        "                    \n",
        "                    v_w_hat = [v_w[i]/(1-BETA2**(epoch+1)) for i in range(num_layers - 1)]\n",
        "                    v_b_hat = [v_b[i]/(1-BETA2**(epoch+1)) for i in range(num_layers - 1)]\n",
        "                    \n",
        "                    self.weights = {str(i+1):self.weights[str(i+1)] - (learning_rate/(np.sqrt(v_w_hat[i])+EPS))*(BETA1*m_w_hat[i]+ (1-BETA1)*deltaw[i]) for i in range(len(self.weights))} \n",
        "                    self.biases = {str(i+1):self.biases[str(i+1)] - (learning_rate/(np.sqrt(v_b_hat[i])+EPS))*(BETA1*m_b_hat[i] + (1-BETA1)*deltab[i]) for i in range(len(self.biases))}\n",
        "\n",
        "                    deltaw = [np.zeros((self.layers[l+1], self.layers[l])) for l in range(0, len(self.layers)-1)]\n",
        "                    deltab = [np.zeros((self.layers[l+1], 1)) for l in range(0, len(self.layers)-1)]\n",
        "             \n",
        "            elapsed = time.time() - start_time\n",
        "\n",
        "            #Y_pred = np.array(Y_pred).transpose()\n",
        "            Y_pred = self.predict(self.X_train, self.N_train)\n",
        "            trainingloss.append(np.mean(CE))\n",
        "            trainingaccuracy.append(self.accuracy(Y_train, Y_pred, length_dataset)[0])\n",
        "            validationaccuracy.append(self.accuracy(self.Y_val, self.predict(self.X_val, self.N_val), self.N_val)[0])\n",
        "\n",
        "            print(\n",
        "                        \"Epoch: %d, Loss: %.3e, Training accuracy:%.2f, Validation Accuracy: %.2f, Time: %.2f, Learning Rate: %.3e\"\n",
        "                        % (\n",
        "                            epoch,\n",
        "                            trainingloss[epoch],\n",
        "                            trainingaccuracy[epoch],\n",
        "                            validationaccuracy[epoch],\n",
        "                            elapsed,\n",
        "                            self.learning_rate,\n",
        "                        )\n",
        "                    )\n",
        "            wandb.log({'loss':np.mean(CE), 'trainingaccuracy':trainingaccuracy[epoch], 'validationaccuracy':validationaccuracy[epoch],'epoch':epoch })\n",
        "            \n",
        "        return trainingloss, trainingaccuracy, validationaccuracy, Y_pred  \n"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_3tVRXJA8Rv",
        "outputId": "8226008e-4c45-466c-b3fc-1a6ac78e502d"
      },
      "source": [
        "import wandb\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import fashion_mnist\n",
        "\n",
        "\n",
        "(trainIn, trainOut), (testIn, testOut) = fashion_mnist.load_data()\n",
        "\n",
        "N_train_full = trainOut.shape[0]\n",
        "N_train = int(0.9*N_train_full)\n",
        "N_validation = int(0.1 * trainOut.shape[0])\n",
        "N_test = testOut.shape[0]\n",
        "\n",
        "\n",
        "idx  = np.random.choice(trainOut.shape[0], N_train_full, replace=False)\n",
        "idx2 = np.random.choice(testOut.shape[0], N_test, replace=False)\n",
        "\n",
        "trainInFull = trainIn[idx, :]\n",
        "trainOutFull = trainOut[idx]\n",
        "\n",
        "trainIn = trainInFull[:N_train,:]\n",
        "trainOut = trainOutFull[:N_train]\n",
        "\n",
        "validIn = trainInFull[N_train:, :]\n",
        "validOut = trainOutFull[N_train:]    \n",
        "\n",
        "testIn = testIn[idx2, :]\n",
        "testOut = testOut[idx2]\n",
        "\n",
        "\n",
        "sweep_config = {\n",
        "  \"name\": \"Bayesian Sweep\",\n",
        "  \"method\": \"bayes\",\n",
        "  \"metric\":{\n",
        "  \"name\": \"validationaccuracy\",\n",
        "  \"goal\": \"maximize\"\n",
        "  },\n",
        "  \"parameters\": {\n",
        "        \"max_epochs\": {\n",
        "            \"values\": [5, 10]\n",
        "        },\n",
        "\n",
        "        \"initializer\": {\n",
        "            \"values\": [\"RANDOM\", \"XAVIER\"]\n",
        "        },\n",
        "\n",
        "        \"num_layers\": {\n",
        "            \"values\": [2, 3, 4]\n",
        "        },\n",
        "        \n",
        "        \n",
        "        \"num_hidden_neurons\": {\n",
        "            \"values\": [32, 64, 128]\n",
        "        },\n",
        "        \n",
        "        \"activation\": {\n",
        "            \"values\": [ 'TANH',  'SIGMOID', 'RELU']\n",
        "        },\n",
        "        \n",
        "        \"learning_rate\": {\n",
        "            \"values\": [0.001, 0.0001]\n",
        "        },\n",
        "        \n",
        "        \n",
        "        \"weight_decay\": {\n",
        "            \"values\": [0, 0.0005,0.5]\n",
        "        },\n",
        "        \n",
        "        \"optimizer\": {\n",
        "            \"values\": [\"SGD\", \"MGD\", \"NAG\", \"RMSPROP\", \"ADAM\",\"NADAM\"]\n",
        "        },\n",
        "                    \n",
        "        \"batch_size\": {\n",
        "            \"values\": [16, 32, 64]\n",
        "        }\n",
        "        \n",
        "        \n",
        "    }\n",
        "}\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config,project='CS6910-DeepLearningFundamentals-Assignment1', entity='rahulsundar')\n",
        "\n"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Create sweep with ID: ph1jjdkr\n",
            "Sweep URL: https://wandb.ai/rahulsundar/CS6910-DeepLearningFundamentals-Assignment1/sweeps/ph1jjdkr\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FyC4fPR8Cwgp"
      },
      "source": [
        "def train():    \n",
        "    config_defaults = dict(\n",
        "            max_epochs=5,\n",
        "            num_hidden_layers=3,\n",
        "            num_hidden_neurons=32,\n",
        "            weight_decay=0,\n",
        "            learning_rate=1e-3,\n",
        "            optimizer=\"MGD\",\n",
        "            batch_size=16,\n",
        "            activation=\"TANH\",\n",
        "            initializer=\"XAVIER\",\n",
        "            loss=\"CROSS\",\n",
        "        )\n",
        "        \n",
        "    wandb.init(config = config_defaults)\n",
        "    #wandb.init(project='CS6910-DeepLearningFundamentals-Assignment1', entity='rahulsundar',config = config_defaults)\n",
        "\n",
        "\n",
        "    wandb.run.name = \"hl_\" + str(wandb.config.num_hidden_layers) + \"_hn_\" + str(wandb.config.num_hidden_neurons) + \"_opt_\" + wandb.config.optimizer + \"_act_\" + wandb.config.activation + \"_lr_\" + str(wandb.config.learning_rate) + \"_bs_\"+str(wandb.config.batch_size) + \"_init_\" + wandb.config.initializer + \"_ep_\"+ str(wandb.config.max_epochs)+ \"_l2_\" + str(wandb.config.weight_decay) \n",
        "    CONFIG = wandb.config\n",
        "\n",
        "\n",
        "    \n",
        "    #sweep_id = wandb.sweep(sweep_config)\n",
        "  \n",
        "\n",
        "    FFNN = FeedForwardNeuralNetwork(\n",
        "        num_hidden_layers=CONFIG.num_hidden_layers,\n",
        "        num_hidden_neurons=CONFIG.num_hidden_neurons,\n",
        "        X_train_raw=trainIn,\n",
        "        Y_train_raw=trainOut,\n",
        "        N_train = N_train,\n",
        "        X_val_raw = validIn,\n",
        "        Y_val_raw = validOut,\n",
        "        N_val = N_validation,\n",
        "        X_test_raw = testIn,\n",
        "        Y_test_raw = testOut,\n",
        "        N_test = N_test,\n",
        "        optimizer = CONFIG.optimizer,\n",
        "        batch_size = CONFIG.batch_size,\n",
        "        weight_decay = CONFIG.weight_decay,\n",
        "        learning_rate = CONFIG.learning_rate,\n",
        "        max_epochs = CONFIG.max_epochs,\n",
        "        activation = CONFIG.activation,\n",
        "        initializer = CONFIG.initializer,\n",
        "        loss = CONFIG.loss\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "    training_loss, trainingaccuracy, validationaccuracy, Y_pred_train = FFNN.optimizer(FFNN.max_epochs, FFNN.N_train, FFNN.batch_size, FFNN.learning_rate)\n",
        " "
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "id": "z4EENrgtDTnu",
        "outputId": "93c25cc0-45c9-4482-b0f9-72e7ffc486c0"
      },
      "source": [
        "wandb.agent(sweep_id, train, count = 100)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: uvmcmb6j with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: TANH\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinitializer: RANDOM\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_epochs: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_hidden_neurons: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: RMSPROP\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.0005\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrahulsundar\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.22<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">neat-sweep-1</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/rahulsundar/CS6910-DeepLearningFundamentals-Assignment1\" target=\"_blank\">https://wandb.ai/rahulsundar/CS6910-DeepLearningFundamentals-Assignment1</a><br/>\n",
              "                Sweep page: <a href=\"https://wandb.ai/rahulsundar/CS6910-DeepLearningFundamentals-Assignment1/sweeps/ph1jjdkr\" target=\"_blank\">https://wandb.ai/rahulsundar/CS6910-DeepLearningFundamentals-Assignment1/sweeps/ph1jjdkr</a><br/>\n",
              "Run page: <a href=\"https://wandb.ai/rahulsundar/CS6910-DeepLearningFundamentals-Assignment1/runs/uvmcmb6j\" target=\"_blank\">https://wandb.ai/rahulsundar/CS6910-DeepLearningFundamentals-Assignment1/runs/uvmcmb6j</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210314_113558-uvmcmb6j</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0, Loss: 8.471e-02, Training accuracy:0.81, Validation Accuracy: 0.82, Time: 44.69, Learning Rate: 1.000e-03\n",
            "Epoch: 1, Loss: 4.791e-02, Training accuracy:0.84, Validation Accuracy: 0.84, Time: 45.65, Learning Rate: 1.000e-03\n",
            "Epoch: 2, Loss: 4.133e-02, Training accuracy:0.87, Validation Accuracy: 0.86, Time: 46.04, Learning Rate: 1.000e-03\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<br/>Waiting for W&B process to finish, PID 982<br/>Program failed with code 1.  Press ctrl-c to abort syncing."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuiqRBuDUhIw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}